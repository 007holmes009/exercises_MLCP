{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.0\n",
    "\n",
    "So today you are finally gonna use some actual machine learning algorithms. Luckily it is quite easy to get started with machine learning in python with the sklearn library you saw last time. Don't get fooled by the simplicity however: A lot is happening under the hood and as you get better you will start to fiddel with the inner gears of it all. For now, however, we are just gonna see that it works and that it does note bite.\n",
    "\n",
    "\n",
    "Now, I have changed a bit in the course plan in regards to the exercises (the schedule and curriculum stays the same otherwise). We will skip Linear regression - since you should be familiar with this one and because we are working with classification anyway. In return we will look on a bit more classifiers than I planned on. Specifically:\n",
    "\n",
    "- Logistic regression\n",
    "- K-nearest Neighbors\n",
    "- Naive Bayes\n",
    "- Decesion Tree\n",
    "- Adaboost\n",
    "- Random forest\n",
    "\n",
    "Non of these are optinal and all should be implemented. However, I do not expect you to understand how each alogirthm works just yet. You should have read about Logistic regression and (especially) Decesion Trees for today and have at least an intuition about what is going on here (If not, make sure you go read up on Decision Trees at some point). Later you will read about Adaboost and Random Forest but that is not on the schedule just yet. For now, you can implement them just fine without knowing how they work (or go watch a youtube video about it). The chapters on K-nearest Neighbors and Naive Bayes are not on the curriculum, so reading up on them is optional (implementing them is not). We will use these last two going forth so some intuition is nice to have (if nothing else, ask youtube).\n",
    "\n",
    "I will update the official course plan to reflect the changes before next Wednesday.\n",
    "\n",
    "For now: import numpy as np, pandas as pd, geopandas as gpd, matplotlib.pyplot as plt, pickle, time and from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Load your pickles `train_set` and `val_set` **(make sure you have run the week3correction.ipynb first)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your data looks as it should. Do whatever you need to convince yourself that everything is as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the good stuff. For simplicity and transparency we'll call our features X and our target y, followed by _train or _val depeding on what set they came from, E.g. X_train will be the features from the train_set. Also, for simplicity, we will only use **binary_best_t1** as target y for now. It is very easy to change to code to more y's (binary_best_t2, binary_best_t3 etc.).\n",
    "\n",
    "Note that if you need to get a lot of features from a pandas dataframe you can collect them in a list and use this list for extraction to a new dataframe. As en exemple:\n",
    "\n",
    "`features = [f1, f2, f3, f4 .... fn]\n",
    "X = df[features]\n",
    "`\n",
    "\n",
    "Lets define X_train, y_train, X_val and y_val. I have predefined the features we want to use here in the list `features` and the target should be `binary_best_t1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = ['gid', 'gwno', 'gwarea', 'year', 'interp_pop_gpw_sum',\n",
    "        'interp_excluded', 'interp_nlights_mean', 'interp_agri_ih',\n",
    "        'interp_barren_ih', 'interp_urban_ih', 'interp_petroleum_y',\n",
    "        'mountains_mean', 'ttime_mean', 'bdist2', 'log_best', \n",
    "        'best', 'low', 'high', 'binary_best', 'ratio_best']\n",
    "\n",
    "X_train = train_set[...]\n",
    "y_train = train_set[...]\n",
    "\n",
    "X_val = val_set[...]\n",
    "y_val = val_set[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.1 Logistic regression:\n",
    "(pretty slow)   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "A simple logistic regression. However, as you will notice if you visit the link above (which you deffenately should) there are alot of (hyper) paremeters to set. I have only set two: random_state=0, max_iter = 512. Check the link if you are crious about waht they do.\n",
    "\n",
    "Note that you do not have to change any code below (but you are of course very welcome to). First lets import the logistic regression classifier from sklean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our classifier (which we here call if `clf`, but you could call it whatever, eg. `logit_clf1` as long as you are consistent) and specify some hyper-parameters. At the same time we also fit it to X_train and y_train. Feel fra to change the specifiy more hyper parameters and see if you can get better results; perhaps try \"penalty\" (see link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter = 512).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with our model trained we can see how good it is at predicting. Note that sklearn classifiers have a number of prediction methods. For exemple `clf.predict()` will give you binary predictions $\\in \\{0,1\\}$ while `clf.predict_proba()` outputs probabilities $\\in (0,1)$. We of course use `clf.predict_proba()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict_proba(X_train)[:,1] # the [:,1] is due to the fact that we get probs for both classes. Which is just p and 1-p. We only need the one.\n",
    "# y_train_pred = clf.predict(X_train) # For binary preidictions\n",
    "\n",
    "y_val_pred = clf.predict_proba(X_val)[:,1]\n",
    "# y_val_pred = clf.predict(X_val) # For binary preidictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice when we use `clf.predict_proba()` on `X_train` we are doing **in-sample prediction**. That is, we test our models ability to predict values of observation which were used to train the model. When use `clf.predict_proba()` on `X_val` (and later `X_test`) we are doing **out-of-sample prediciton**. That is, we test our models ability to predict values of observation which our model has never seen before.   \n",
    "\n",
    "Now lets look at some results. We just use AUC, AP and the Brier score here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_train = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "AP_train = metrics.average_precision_score(y_train, y_train_pred)\n",
    "BS_train = metrics.brier_score_loss(y_train, y_train_pred)\n",
    "\n",
    "AUC_val = metrics.roc_auc_score(y_val, y_val_pred)\n",
    "AP_val = metrics.average_precision_score(y_val, y_val_pred)\n",
    "BS_val = metrics.brier_score_loss(y_val, y_val_pred)\n",
    "\n",
    "print(f'\\tTrain\\tVal\\nAUC: \\t{AUC_train:.3f}\\t{AUC_val:.3f}\\nAP: \\t{AP_train:.3f}\\t{AP_val:.3f}\\nBS: \\t{BS_train:.3f}\\t{BS_val:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now compare these results with one or both of the baseline models - either explicitly here or just look in 2B. How does logistic regression compare?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.2 K Nearest Neighbors  \n",
    "(A bit slow)   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "Take a moment to look at the documentation. Now lets load the  K Nearest Neighbors classifier from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3, n_jobs = -1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the model, use it to create `y_train_pred` and `y_val_pred` - just as you did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print the results as you did above. Note that you are gonna do this a number of times so it might be worth it to create a function - as oppose to copy/past code all the way down. You don't have to but I do recommend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are we doing? Better than baselines?**   \n",
    "**Better than logistic?**  \n",
    "**Do you notice any difference in performence on the training set and the validation set?**  \n",
    "**If so, why o you think there is a difference?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.3 Naive Bayes\n",
    "(pretty fast)   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "\n",
    "We load the Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create y_train_pred and y_val_pred using the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anything to note?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.4 Decision Tree\n",
    "(pretty fast)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state = 0).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any difference between the performance on respectively the trainig set and the validation set?**   \n",
    "**If so, why do think this is?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.5 Adaboost\n",
    "(Speed depends on n_estimators and more, but relatively fast at low n_estimators)  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "As always take a second to look at the documentation. However, don't worry to much about it: We'll learn more about this algorithm later. Load the Ada boost classifier from sklearn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=128, random_state = 0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create y_train_pred and y_val_pred using the Ada boost classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the AUC, AP and Brier score for both validation set and train set as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you here notice in regards to the difference between the performance on train and validation set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.6 Random forest\n",
    "(Speed depends on n_estimators and more, but relatively fast at low n_estimators)  \n",
    "https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-treesa   \n",
    "\n",
    "Again, look at the documention but don't worry to much about it yet. Notice, however, that this algorithm is closly related to the Decision Tree used above. Indeed it is a algorithm which aims to use the strengths of the Decision tree without inducing massive overftting. Let load the Random Forest classifier from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=32, n_jobs = -1, random_state = 0).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create y_train_pred and y_val_pred using the Random Forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results as you have above (AUC, AP, BS for both training and validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.7:\n",
    "\n",
    "Try plotting the ROC and PR curve as seen in 2B/week4. Feel free to use results from whatever classifier you want to. Also try and see if you can plot the PR curve (validation) from two or more models and/or baselines in the same plot for comparison. These links might (and might not) serve as inspiration:   \n",
    "\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n",
    "- https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3A.optional\n",
    "\n",
    "Choose one or more models and use them to predict all our targets/y's: So `binary_best_t1`, `binary_best_t2`, `binary_best_t3`, `binary_best_t4` and `binary_best_t5`. Now compare the results as we move futher into the furture - do they change? You can compare by printing individuel scores/plots of each binary_best_tn or by creating a \"trend\" plot as seen in 2A.6/week3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you'r done. Good work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
