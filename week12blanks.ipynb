{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That last one\n",
    "\n",
    "We are not gonna do a lot of new things today. Mostly we'll just combine what you have learned in order to make our final prediction. It is important that you have successfully run week12split.ipynb (in geo_env) before starting on this exercises (in xgb_env). The reason is that we a going to use the pickles generated in week12split.ipynb Lets get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B.1 Getting Started\n",
    "Load y_train_week12.pkl, X_train_week12.pkl, y_val_week12.pkl, X_val_week12.pkl which you created in week12split.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = ...\n",
    "y_train = ...\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = ...\n",
    "X_train = ...\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = ...\n",
    "y_val = ...\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = ...\n",
    "X_val = ...\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do what ever you need to make sure the data is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have included xcoord and ycoord we can plot our plots a different way than usual. It is slightly more involved, but way faster and way more flexible. What you see below is simply a scatter plot where x is xcoord and y is ycoord. The scatters are then colored (c) in regards to some feature. I used 'log_best_decay10' but fell free to experiment. Note that we need to specific year for each input or the scatter plot will end up containing all years. I've sat makers to 's' (squares) so the scatter points mimics the pixels we usually plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [22,10])\n",
    "plt.scatter(X_train[X_train['year'] == 2012]['xcoord'], X_train[X_train['year'] == 2012]['ycoord'], c = X_train[X_train['year'] == 2012]['log_best_decay10'], s=1, marker='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B.2 Feature Selection\n",
    "\n",
    "We looked at feature selection before. Here we are gonna choose a subset of features make sure we can save some training time (there is gonna be a lot of that below) while not compromising our predictive capabilities. We are gonna use an XGB model for this purpose.  \n",
    "\n",
    "Start by defining and fitting a XGBClassifier on X_train and y_train\\['binary_best_t1'\\]. Set number of estimators to 16. This is very low, but you can always go back and set it higher once you have the whole exercise running. Now create and print your training predictions and validation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ... # 16 estimators is very low. Try 32, 64 or 128 if you have the time\n",
    "model.fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = ...\n",
    "y_val_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_train = ...\n",
    "AP_train = ...\n",
    "BS_train = ...\n",
    "\n",
    "AUC_val = ...\n",
    "AP_val = ...\n",
    "BS_val = ...\n",
    "\n",
    "print(f'\\tTrain\\tVal\\nAUC: \\t{AUC_train:.3f}\\t{AUC_val:.3f}\\nAP: \\t{AP_train:.3f}\\t{AP_val:.3f}\\nBS: \\t{BS_train:.3f}\\t{BS_val:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you of course remember how we created the importance plots for Random Forest and XGboost. We can use such importance measures the rank our features choosing the best ones. To know how many to included we can start be including the most important, then the next most important and so forth. We stop when our predictive capabilities does not get better of if they start to get worse. The code below prints out the 15 most important features given the model we just fitted. They are ordered so the last on is the most important, the second to last the second most important one and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = model.feature_importances_.argsort()\n",
    "X_train.columns[sorted_idx][-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This here loop will iterate over the 20 most important features and create increasingly large models (could be all features; 20 is to save time). We save (and print) the evaluation metrics as the model gets bigger. No code needs changing but make sure you understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "AUC_train_list = []\n",
    "AP_train_list = []\n",
    "BS_train_list = []\n",
    "\n",
    "AUC_val_list = []\n",
    "AP_val_list = []\n",
    "BS_val_list = []\n",
    "\n",
    "for i in np.arange(1,21,1):\n",
    "    \n",
    "    selected_features = X_train.columns[sorted_idx][-i:]\n",
    "    X_train_slim = X_train[selected_features]\n",
    "    X_val_slim = X_val[selected_features]\n",
    "    \n",
    "    model_tmp = XGBClassifier(n_estimators=16, random_state = 42)# 16 estimators is very low. Try 32, 64 or 128 if you have the time\n",
    "    model_tmp.fit(X_train_slim, y_train['binary_best_t1'])\n",
    "    \n",
    "    y_train_pred = model_tmp.predict_proba(X_train_slim)[:,1]\n",
    "    y_val_pred = model_tmp.predict_proba(X_val_slim)[:,1]\n",
    "    \n",
    "    AUC_train_list.append(metrics.roc_auc_score(y_train['binary_best_t1'], y_train_pred))\n",
    "    AP_train_list.append(metrics.average_precision_score(y_train['binary_best_t1'], y_train_pred))\n",
    "    BS_train_list.append(metrics.brier_score_loss(y_train['binary_best_t1'], y_train_pred))\n",
    "\n",
    "    AUC_val_list.append(metrics.roc_auc_score(y_val['binary_best_t1'], y_val_pred))\n",
    "    AP_val_list.append(metrics.average_precision_score(y_val['binary_best_t1'], y_val_pred))\n",
    "    BS_val_list.append(metrics.brier_score_loss(y_val['binary_best_t1'], y_val_pred))\n",
    "    \n",
    "    print(f'{i}/15 AP (val): {AP_val_list[i-1]}, using {selected_features}\\n')\n",
    "    \n",
    "\n",
    "end_time = time.time()\n",
    "run_time = (end_time - start_time)/60\n",
    "print(f'minutes to run: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the printout above you should have a pretty good idea of when the model performance saturates. Lets make this even more explicit by plotting how the evaluation metrics evolved as functions of included features. No code needs changing, but feel free the optimize and prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(1,21,1)\n",
    "\n",
    "plt.title('AUC')\n",
    "plt.plot(grid, AUC_train_list, '--', label = 'train')\n",
    "plt.plot(grid, AUC_val_list,  '-', label = 'val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('AP')\n",
    "plt.plot(grid, AP_train_list, '--', label = 'train')\n",
    "plt.plot(grid, AP_val_list, '-', label = 'val')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('BS')\n",
    "plt.plot(grid, BS_train_list, '--', label = 'train')\n",
    "plt.plot(grid, BS_val_list, '-', label = 'val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets slim down our X_train and X_val to X_train_slim and X_val_slim. Simply choose a subset of features given the results above. You can choose the number manually or just take the 'argmax' to get the best number given some metric. No code needs changing, but as it is now we take the number which optimized the AP (val)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = np.argmax(np.array(AP_val)) # you can take the number that optimizes some metric, here AP.\n",
    "\n",
    "# optim = 5 # you can set it by hand\n",
    "\n",
    "\n",
    "selected_features = X_train.columns[sorted_idx][-optim:]\n",
    "\n",
    "print(selected_features)\n",
    "\n",
    "X_train_slim = X_train[selected_features]\n",
    "X_val_slim = X_val[selected_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your \"slim\" data by defining, fitting and evaluating an XGB classifier to y_train\\['binary_best_t1'\\] - is it as good as a model using all features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on - until 5B.5 - when I say fit a model i mean fit a model to X_train_slim and y_train\\['binary_best_t1'\\] unless something else is explicitly stated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B.3 Grid Search (again)\n",
    "\n",
    "Here we are going to do grid search. This is gonna be pretty much like last time except: we are only gonna use full grid search and beyond XGboot your are also gonna apply grid search on a KNeighbors classifier, a naive Bayes classifier (BernoulliNB), an AdaBoost classifier and a Random Forest classifier. I have proposed which hyper parameters and ranges you survey, but feel free to experiment. The first one is a freebie so no code needs to be changed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 2 # 2 folds is very low, try 3, 6 or 9 if you have the time\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBparams = {\n",
    "        'max_depth': [4, 5],\n",
    "        'learning_rate': [0.1, 0.5],\n",
    "        'gamma': [0.1, 1],\n",
    "        'reg_lambda': [0.01, 0.1]\n",
    "        }\n",
    "\n",
    "XGBmodel = XGBClassifier(n_estimators=16, random_state = 42)# 16 estimators is very low. Try 32, 64 or 128 if you have the time\n",
    "\n",
    "XGBgrid = GridSearchCV(estimator=XGBmodel, param_grid=XGBparams, scoring='average_precision', n_jobs=-1, cv=skf.split(X_train_slim, y_train['binary_best_t1']), verbose=3 )\n",
    "\n",
    "start_time = time.time()\n",
    "XGBgrid.fit(X_train,y_train['binary_best_t1'])\n",
    "end_time = time.time()\n",
    "\n",
    "run_time = (end_time - start_time)/60\n",
    "\n",
    "print(f'minutes to run: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the new fund parameters to define a model. Don't fit or evaluate it just yet. We'll use it soon enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBmodel_final = XGBClassifier(n_estimators = 16,  # 16 estimators is very low. Try 32, 64 or 128 if you have the time\n",
    "                       reg_lambda = XGBgrid.best_params_['reg_lambda'],\n",
    "                       max_depth = XGBgrid.best_params_['max_depth'], \n",
    "                       learning_rate = XGBgrid.best_params_['learning_rate'],\n",
    "                       gamma =  XGBgrid.best_params_['gamma'], \n",
    "                       random_state = 42)\n",
    "\n",
    "print(XGBmodel_final.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do a grid search on a KNeighborsClassifier. The procedure is the same as above and I have supplied a dictionary with parameter ranges to be tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNparams = {\n",
    "        'n_neighbors' : [1,2,4],\n",
    "        'weights': ['uniform', 'distance']\n",
    "        }\n",
    "\n",
    "KNNmodel = KNeighborsClassifier()\n",
    "\n",
    "KNNgrid = GridSearchCV(estimator=..., param_grid=..., scoring='average_precision', n_jobs=-1, cv=skf.split(..., ...), verbose=3 )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "KNNgrid.fit(..., ...)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "run_time = (end_time - start_time)/60\n",
    "\n",
    "print(f'minutes to run: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define a model using the new found parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNmodel_final = KNeighborsClassifier(n_jobs = -1, \n",
    "                                      n_neighbors = KNNgrid.best_params_['n_neighbors'], \n",
    "                                      weights =  KNNgrid.best_params_['weights'])\n",
    "\n",
    "print(KNNmodel_final.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we trun to the Naive Bayes classifier. Do a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NBparams = {\n",
    "        'alpha' : [0.1, 0.2, 0.5, 1],\n",
    "        'fit_prior': [True, False]\n",
    "        }\n",
    "\n",
    "NBmodel = BernoulliNB()\n",
    "\n",
    "NBgrid = ...\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "NBgrid.fit(... , ...)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "run_time = (end_time - start_time)/60\n",
    "\n",
    "print(f'minutes to run: {run_time:.2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model using the new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBmodel_final = BernoulliNB(alpha = NBgrid.best_params_['alpha'],\n",
    "                            fit_prior = NBgrid.best_params_['fit_prior'])\n",
    "\n",
    "print(NBmodel_final.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ADAboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAparams = {\n",
    "        'base_estimator' : [DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3)],\n",
    "        'learning_rate': [0.5, 1]\n",
    "        }\n",
    "\n",
    "ADAmodel = ...  # set n_estimators at 16. It is very low so try 32, 64 or 128 if you have the time\n",
    "\n",
    "ADAgrid = ...\n",
    "\n",
    "start_time = time.time()\n",
    "ADAgrid.fit(..., ...)\n",
    "end_time = time.time()\n",
    "\n",
    "run_time = (end_time - start_time)/60\n",
    "\n",
    "print(f'minutes to run: {run_time:.2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAmodel_final = AdaBoostClassifier(n_estimators=16, random_state = 42, # 16 estimators is very low. Try 32, 64 or 128 if you have the time \n",
    "                                    base_estimator = ADAgrid.best_params_['base_estimator'], \n",
    "                                    learning_rate = ADAgrid.best_params_['learning_rate'])\n",
    "\n",
    "print(ADAmodel_final.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RFparams = {\n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "        'max_depth': [3, 4],\n",
    "        'class_weight': [None, 'balanced']\n",
    "        }\n",
    "\n",
    "\n",
    "RFmodel = ...  # set n_estimators at 32. It is very low so try 64 or 128 later if you have the time\n",
    "\n",
    "RFgrid = ...\n",
    "\n",
    "start_time = time.time()\n",
    "RFgrid.fit(X_train,y_train['binary_best_t1'])\n",
    "end_time = time.time()\n",
    "\n",
    "run_time = (end_time - start_time)/60\n",
    "\n",
    "print(f'minutes to run: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel_final = RandomForestClassifier(n_estimators=32, n_jobs = -1, random_state = 42,  # 32 estimators is very low. Try 64 or 128 if you have the time\n",
    "                                 criterion = RFgrid.best_params_['criterion'], \n",
    "                                 max_depth = RFgrid.best_params_['max_depth'], \n",
    "                                 class_weight = RFgrid.best_params_['class_weight'])\n",
    "\n",
    "print(RFmodel.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B.4 Le grand ensemble\n",
    "\n",
    "Now, above you saved a number of \"final\" models. We are gonna use those now the create a larger voting ensemble as you also did in weeek 6. You don't need to use any weighting scheme just the \"final\" models defined after each grid search above. After the voting classifier is defined fit it to X_train_slim and y_train\\['binary_best_t1'\\]. Evaluate the voting ensemble model and print out the relevant training and validation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = ...\n",
    "eclf = eclf.fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = ...\n",
    "y_val_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_train_eclf = ...\n",
    "AP_train_eclf = ...\n",
    "BS_train_eclf =...\n",
    "\n",
    "AUC_val_eclf = ...\n",
    "AP_val_eclf = ...\n",
    "BS_val_eclf = ...\n",
    "\n",
    "print(f'\\tTrain\\tVal\\nAUC: \\t{AUC_train_eclf:.3f}\\t{AUC_val_eclf:.3f}\\nAP: \\t{AP_train_eclf:.3f}\\t{AP_val_eclf:.3f}\\nBS: \\t{BS_train_eclf:.3f}\\t{BS_val_eclf:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5b.4 Trend\n",
    "\n",
    "Now we a gonna see how our model does further out into the future. So at y_t2, y_t3, y_t4 and y_t5. Note that, if time permitted, we would do all optimization steps again with respect to each y, since we can't be sure it does not change. Today, however, we assume not much would change.\n",
    "\n",
    "The way we do this is similar to the feature selection for-loop step above. Instead of iterating over different Xs we just iterate over the different ys. No code needs to be change below, but feel free to optimize and experiment.\n",
    "\n",
    "Note, that this might take a loooonge time to run.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "AUC_train_list = []\n",
    "AP_train_list = []\n",
    "BS_train_list = []\n",
    "\n",
    "AUC_val_list = []\n",
    "AP_val_list = []\n",
    "BS_val_list = []\n",
    "\n",
    "for i,j in enumerate(y_train.columns):\n",
    "\n",
    "    model_tmp = VotingClassifier(estimators=[('XGB', XGBmodel), ('NB', NBmodel), ('ADA', ADAmodel), ('RF', RFmodel), ('KNN', KNNmodel)], voting='soft', n_jobs = -1)\n",
    "    model_tmp.fit(X_train_slim, y_train[j])\n",
    "    \n",
    "    y_train_pred = model_tmp.predict_proba(X_train_slim)[:,1]\n",
    "    y_val_pred = model_tmp.predict_proba(X_val_slim)[:,1]\n",
    "    \n",
    "    AUC_train_list.append(metrics.roc_auc_score(y_train[j], y_train_pred))\n",
    "    AP_train_list.append(metrics.average_precision_score(y_train[j], y_train_pred))\n",
    "    BS_train_list.append(metrics.brier_score_loss(y_train[j], y_train_pred))\n",
    "\n",
    "    AUC_val_list.append(metrics.roc_auc_score(y_val[j], y_val_pred))\n",
    "    AP_val_list.append(metrics.average_precision_score(y_val[j], y_val_pred))\n",
    "    BS_val_list.append(metrics.brier_score_loss(y_val[j], y_val_pred))\n",
    "    \n",
    "    print(f'{i+1}/5 AP (val): {AP_val_list[i]}, y: {j}\\n')\n",
    "    \n",
    "\n",
    "end_time = time.time()\n",
    "run_time = (end_time - start_time)/60\n",
    "print(f'minutes to run: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(1,6,1)\n",
    "\n",
    "plt.figure(figsize = [15,5])\n",
    "\n",
    "plt.title('AUC')\n",
    "plt.plot(grid, AUC_train_list, '--', label = 'train')\n",
    "plt.plot(grid, AUC_val_list,  '-', label = 'val')\n",
    "plt.xlabel('years into the future')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = [15,5])\n",
    "\n",
    "plt.title('AP')\n",
    "plt.plot(grid, AP_train_list, '--', label = 'train')\n",
    "plt.plot(grid, AP_val_list, '-', label = 'val')\n",
    "plt.xlabel('years into the future')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = [15,5])\n",
    "\n",
    "plt.title('BS')\n",
    "plt.plot(grid, BS_train_list, '--', label = 'train')\n",
    "plt.plot(grid, BS_val_list, '-', label = 'val')\n",
    "plt.xlabel('years into the future')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to add the baselines models (persistence and ratio) to the three plots above for some context - how are we doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B.5 TEST TIME\n",
    "\n",
    "Now do the exact same thing you did above (5B.4) but now using the X_train_ttime, y_train_ttime, y_test and X_test. Not that in an authentic setting the test set can only be used once. So if you have any fine tuning you should go and do it now before evaluation against the test set. The scores you get a probably a bit worse than above but hopefully it is not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
