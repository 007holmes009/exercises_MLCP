{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Part 1 - The Phantom Burden\n",
    "\n",
    "Before the actual Exam assignment this little introduction is meant to give some initial guidance which might serve to make the assignment more manageable as you go forth. As such, return here when in doubt. Naturally you are also welcome to write me or your peers on discord - though there are some limits to what I will answer. Regarding your peers on discord: As always almost everything is allowed so feel free to share ideas, explanations, elaborations, interpretations, links, videos, tutorials, and so on, **but please do refrain from sharing you own code solutions directly**. Naturally I can't - and don't want to - stop you from sharing code solutions in general, but please take the time to understand why your code was not working while your peers did work. This also holds true if you find inspiration in the exercises which you provided feedback for throughout the course. Use them plenty but please do reflect upon what is done. Lastly, it goes without saying that copy-pasting from your own past assignments is 100% legit (even expected).\n",
    "\n",
    "Moving on, the first subsection below will outline what is expected and covered in this exam assignment. The second subsection is meant to give you some inspiration should you wish to aim for a high-end-grade. The last subsection before the actual exam gives some advise regarding what to do if one or more exercises proves insurmountable.\n",
    "\n",
    "\n",
    "### Expectations and topics to be covered:\n",
    "In this Exam we will cover all the hard - often overlooked - work that goes into making the raw data ready for our Machine Learning algorithms. Specifically acquiring and merging different data sources, preprocessing steps such as interpolations and extrapolations to handle missing values, feature engineering including theoretical considerations, and lastly splitting your data into train, validation and test set. As promised, all topics and related code needed to complete this exam have been covered in the weekly assignments. Below you will find that I am rather explicit regarding what I want you to do. That being said, there are also a lot of steps that should be implicitly inferred; like which libraries you have to import or minor steps from A to B. Here, the weekly exercises should serve as your guide. If you, when concluding this assignment, think that your code is running (somewhat) smoothly and that you have covered all mentioned topics, then it is likely that you will receive a passing, hopefully even satisfying, grade.\n",
    "\n",
    "That being said, I will expect you to explain and comment slightly more on your work than we usually do in the weekly assignments. This is to facilitate some reflection on your part regarding your implementations and decisions. I will not always provide the well-known **Questions** but expect you to take some initiative yourselves. In this regard, the exam differs from the weekly exercises. However, while explanations, reflections, argumentations and justifications have not been required up until now, you should be more than qualified to face such challenges given you general academic credentials. References are not required. Your own words and thoughts will do just fine.\n",
    "\n",
    "It should also be noted, that the order of the tasks in the exam does not necessarily align with the order of the weekly assignments. Often they do, but not always. This is because I aim to let the exam mimic a more conventional data science/machine learning workflow. Naturally, I am aware that the code in your weekly exercises are not optimised for such flow and thus I do expect it to appear a bit artificial. This is fine, don't worry about it. Even more so, due to the fact that we would normally split up the tasks, presented below, in multiple smaller scripts. Here everything is instead expected to be submitted in one jupyter notebook. As such I also advise you to make \"savepoints\" along the way where you save and then reload you pickles - so you don't have to run computational expensive operations more times than necessary.   \n",
    "\n",
    "And while on the subject on artificiality, I also expect a lot fewer \"sanity checks\" presented in you final hand-in - compared to what is actually prudent. The point is not that you should run fewer sanity checks than normal - you should run plenty to make sure you capture any odd mistakes before they creep in everywhere. Checking, doublet checking and triple checking (n-checking!) is paramount when handling and manipulating data. My request is simply that when you make you final submission, most of these checks should be removed from the notebook so that your product appears somewhat concise and readable. Thus, do not display 20 map plots in a row, overly long outputs such as the first 142 observations of a dataframe or the full output of a list with 999 entries. Try to make it look like something you would not be ashamed to make public on github.\n",
    "\n",
    "\n",
    "### The perfect assignment:\n",
    "Does naturally not exist. But if you have covered the topics above and you follow the recommendations below your chances of getting a good grade will be very high. Me first recommendation follows from the section above: Good written explanations and explicit reflections goes a long way. I am not expecting a novel, just short, on-point, text pieces explaining the jist of what you do and, when relevant, a reflection or two to boot. What naturally makes such efforts even better are references to the curriculum, especially the articles. References beyond the curriculum are also appreciated. This can be in regards to method an subjects alike and it does not have to be a lot. Just showing that you can connect you knowledge to the literature is enough. Also, when applicable explicit math-statements make operations less ambiguous which is appreciated - note that I will reward honest efforts even if the math is slightly off.\n",
    "\n",
    "Another thing which will be greatly appreciated is good \"code hygiene\". That is slick and concise code with little to no redundant python objects haunting your code. The easiest way to achieve this is to put your operations into loops, functions or both when applicable. Even if you do not do it everywhere, your efforts will surely be noticed. As will well-commented code. Again, not to much, but being able to read in the code (#via some comment) what different pieces do is greatly appreciated. And if you do create functions, consider creating a description in the beginning (\"\"\"Such as some function description\"\"\").  \n",
    "\n",
    "Lastly; beautiful plots. This is your chance to score some pretty cheap points, since I do enjoy nice plots a lot and python plots are pretty easy to beautify. Note that I say beautify not *pimp*. The reason behind my choice of words is that less is often more when it comes to data visualisations. Background colours should be avoided if they serve no purpose. Grids should only be used when necessary, Readable and meaningful scales on y and x axis are mandatory. Indeed if any text appear in or around a given plot this text should be readable. If text is not clearly readable it is redundant almost by definition. The go-to-guy on data visualisation is Edwar Tufte. Here is a blog (not by himself though) with some of his points https://moz.com/blog/data-visualization-principles-lessons-from-tufte. As a last note on plots; unnecessary 3D plots, Word Clouds and Pie charts are *not* appreciated.\n",
    "\n",
    "### When it does not work:\n",
    "Shit happens - such is life. If you find yourself close to deadline and unable to implement some code try to show/tell me what you would have done. Make comments in your code telling what each part do, where you think the problem is and what you would have liked the code to do. Also, make comments outside the code elaborating on the issue(s). Why do you think the problem persist? What have you tried to do about it? And just as importantly; what is it that you are trying to do with the code in the first place? If you do know what you are trying to do, please write so and write what the expected output should have been. Offer me explanations. It is just as important (if not more in practice) that you know what you want to implement as it is that you know how to implement it (youtube will teach you the latter in due time). Also, there's no shame in reaching out to either me or your peers. I am not keeping any scores regarding who asks questions on discord - at least not for grading purpose or to hold it against you.\n",
    "\n",
    "Lastly, remember that I have uploaded pickles for the \"full_df\" and the test, validation and train set (pre feature engineering). You are allowed the use these for the exam if you get stuck or you own data acts up. But remember to note it if you do so, including an explenation as to why and where - in order avoid too much confusion on my part. Naturally, if you just use the data for sanity checks doing the coding process, e.g. to see whether this data produce the same results as you own, you don't need to note anything. \n",
    "\n",
    "That's all - may the force be with you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1\n",
    "Download the UCDP Georeferenced Event Dataset (GED) Global version 20.1 (.csv). Read it into python as a Pandas Dataframe. Print the shape of the dataframe. Display 7 random samples. And display the \"info\" of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2\n",
    "\n",
    "Plot the aggregated number of yearly fatalities (\"low\", \"best\" and \"high\") from the UCDP using matplotlib.pyplot. To do this you must first aggregated (groupby(by=..).sum()) the the UCDP data at a yearly level and then plot the different fatalities counts in one plot. If you reckon it would be more suitable to plot the fatalities logged, please do so and write some thoughts about why this might make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3\n",
    "\n",
    "Download the PRIO grid shape files, the PRIO grid static variables and the PRIO grid early variables (1989 to 2014). Load the static and dynamic variables as pandas dataframes and the PRIO shape files as a geopandas dataframe. Merge the PRIO grid and the static variables (\"on\" the grid identifier; gid) into a new dataframe. To check that it worked, plot a map of the merged dataset showing the static feature \"mountains_mean\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4\n",
    "\n",
    "Now we want to merge the new geopandas dataframe holding the static features with the UCDP data. However, the UCDP spans over years and our geopandas dataframe does not. Thus, we need to \"stretch\" the geopandas dataframe across the same span of years as the UCDP. This can be done in a lot of more or less elegant ways. One of which I previously provided you with. \n",
    "\n",
    "Having made sure the UCDP and the geopandas dataframe match on the temporal dimension you should merge the geopandas data frame and your UCDP data (on 'gid' and 'year') into a new dataframe. To make sure it worked, create the features \"log_low\", \"log_best\" and \"log_high\". Now create two map-plots displaying one of these features. One map-plot for 1994 and one for 2019. Use the same feature for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5\n",
    "\n",
    "Now, to complete our data set we only need to add the dynamic features. But as you know, the PRIO dynamic variables are riddled with missing values. Lets get rid of these first using linear interpolation and extrapolation (fill backward and fill forward). Again, this can be done in many ways, one which I have already provided you. \n",
    "\n",
    "To save computation time and keep our effort concise, you only need to concentrate this interpolation/extrapolation effort on a subset of features (features_of_interest):\n",
    "- ['nlights_mean', 'pop_gpw_sum', 'agri_ih', 'barren_ih', 'urban_ih', 'petroleum_y','excluded']   \n",
    "\n",
    "Test whether you succeeded. Do this by plotting the trend of the (interpolated) \"nlights_mean\" features throughout the years. Do this for 8 more or less random gids in the same plot (for a more interesting plot make sure you draw gids with at least some non-zero values on the (interpolated) \"nlights_mean\" feature).  \n",
    "\n",
    "Please reflect briefly on the potential dangers and pitfalls of using interpolation and extrapolation. Also, feel free to note if there are any features left out that you would consider using for furture efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6\n",
    "\n",
    "Now merge the interpolated dataframe with the geopandas dataframe containing the grid, the static features and the UCDP data. Thus, all data sources should now have been merged into new new dataset (which we in the weekly exercise denoted full_df.pkl). To keep our effort somewhat concise create a data set (in the weekly exercises called final_df) which only contains the following features:\n",
    "\n",
    "- [['gid', 'gwno', 'gwarea', 'year','interp_pop_gpw_sum', 'interp_excluded', 'interp_nlights_mean', 'interp_agri_ih', 'interp_barren_ih', 'interp_urban_ih', 'interp_petroleum_y', 'mountains_mean', 'ttime_mean', 'bdist2', 'geometry', 'log_best', 'best', 'low', 'high']]\n",
    "\n",
    "\n",
    "Check your \"final\" dataframe by printing the shape, displaying 9 random samples, displaying the \"info\" and create two map plots of your own choosing. Comment on what we see and whether everything is in order. This is also a very prudent time to place a \"savepoint\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7\n",
    "\n",
    "Now add to your \"final\" dataframe the feature/target \"binary_best\" which should be a binary representation of \"best\" where $\\text{\"binary_best\"} = 1 \\text{ if } \\text{ \"best\" } > 0 \\text{ and } \\text{ \"binary_best\" } = 0 \\text{ if } \\text{ \"best\" } = 0$. Afterwards create the \"sliding window\" targets \"binary_best_t1\", \"binary_best_t2\", \"binary_best_t3\", \"binary_best_t4\", \"binary_best_t5\". This should be done by \"shifting\" the \"binary_best\" feature/target -1, -2, -3, -4, and -5 years respectively. Remember here to sort values by \"year\" and group by \"gid\". Now please make two map plots. One of \"binary_best\" in 1994 and one of \"binary_best_t1\" in 1993. What do we see and what do we expect to see? \n",
    "\n",
    "If everything is in order, create a dataframe only including observations from before 2015 (2104 and down. So 2015 and beyond should be discarded). Comment on why we do this. Now print the shape of this new dataframe. What do you see compared to when you printed the shape in 1.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8\n",
    "Elaborate on the following (so no code here): Which columns/variables are now our targets and how should they be understood more substantially? And what about \"binary_best\" is this now a target or a feature? And what connection does \"binary_best\" have to the concept of baseline models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.9\n",
    "\n",
    "Now check if there are any features in your dataset which still contains missing values. This can be done with or without the use of map plots. If some features are still containing missing values fill these missing values using some appropriate scheme (backwards fill, mean interpolation, forward fill or some such). Argue why your filling strategy can be (more or less) justified. You are advised to point out potential dangers here.\n",
    "\n",
    "Now check if any features are still containing missing values. This should not be the case by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10\n",
    "\n",
    "(This next step may appear to be in a somewhat artificial order. This is due to the rhythm of the weekly exercises and thus the code I have provided for you. Don't worry about it.) \n",
    "\n",
    "Create a test set containing all observations from 2014. Create a validation set containing all observations from 2013. And create a training set containing all remaining observations (so 2012 and down). Explain why we do this? Elaborate a bit on the merits of out-of-sample prediction vs. in-sample-prediction. Also, comment on what we should be mindful of when doing our-of-sample-predictions in time series data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.11\n",
    "\n",
    "Now, using code from week3correction.ipynb create the feature \"ratio_best\" in you train, validation and test set. Comment on the following: Other than being used as a feature in larger models, what important purpose does \"ratio_best\" serve?\n",
    "\n",
    "This is also a good time for a second savepoint if you have not already created on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.12\n",
    "\n",
    "Using feature engineering create the follow features for the training set and validation set (don't worry about the test set for now):\n",
    "\n",
    "- past_fatalities\n",
    "- past_magnitude\n",
    "- past_events\n",
    "- past_fatalities_country\n",
    "- past_magnitude_country\n",
    "- past_events_country\n",
    "- past_fatalities_country_Narea\n",
    "- norm_past_fatalities_country\n",
    "- past_magnitude_country_Narea\n",
    "- norm_past_magnitude_country\n",
    "- past_events_country_Narea\n",
    "- norm_past_events_country\n",
    "- norm_interp_pop_gpw_sum_country\n",
    "- past_fatalities_country_Npop\n",
    "- past_magnitude_country_Npop\n",
    "- past_events_country_Npop\n",
    "- best_decay5\n",
    "- best_decay10\n",
    "- log_best_decay5\n",
    "- log_best_decay10\n",
    "- cell_light_Pcap\n",
    "- interp_nlights_mean_country\n",
    "- country_light_Pcap\n",
    "- country_light_Area\n",
    "- interp_nlights_mean_norm\n",
    "- interp_nlights_mean_country_norm\n",
    "- country_light_Pcap_norm\n",
    "- country_light_Area_norm\n",
    "- low_ratio_light\n",
    "- low_ratio_light_Pcap\n",
    "- low_ratio_light_Area\n",
    "\n",
    "The what, how and why can be found in the weekly exercises. I advise you to use the complete function provided by me in week75.ipynb for this step, but you are free to do it as you wish.\n",
    "\n",
    "Now, looking at the code and discribtions in the relevant weekly exercises, give a short introduction and justification of each feature. What do they represent and why do we included them? You own interpretation and inputs will be appreciated. Also, argue for other potentially rewarding features we which could have constructed.\n",
    "\n",
    "Lastly, save you new train and validation sets (which should include the newly engineered features) as pickles. These are the pickels you will be using for exam_part2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.13\n",
    "\n",
    "Pat yourself on the back. Thats it. Well done! See you for part two."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
