{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle engineered features\n",
    "\n",
    "Before starting on the actual assigment 4B, this script will cover the gap between last weeks 4A and this weeks 4B. That is, last week we did not go ahead and pickle the data after creating our new features. This we will do here. We will do so in regards to both the `train_set` and the `val_set`. The code below is also construted so we can create a `train_set` (which includes the data from the `val_set`) and a final `test_set`. This last bit will not be relevant before later in the course, but you should know that the functionality is already here. \n",
    "\n",
    "Now, to understand what is going on, it should be noted that we would often implent most of our feature engeneering on the full data set before splitting it into train, val and test. Or indeed it might be an iterative process where we se what works before going back and tweaking it. In this course we go a bit back and forth, but this is not as much for tweaking. Instead it is because I wanted you to start predicting and evaluationg models as soon as possible, which required us to create a train and a validation set.\n",
    "\n",
    "However, we naturally need the same features in our `val_set`/`test_set` as we have in our `train_set`. As such it is easier to create the features before splitting into `train/val/test`. Instead of importing the full_df the function below concatenate the train and validation (and potentially also the test) set back togheter and run all the feature engeneering we did last time on the full data set. Afterwards it splits the data again into train and validation set (or train and test if `test_time = True`).\n",
    "\n",
    "A couple of things to be aware of when doing such operations: Be aware if you have any features which might send informaiton \"back through time\", such as \"total fatalities count across all years\". This is analog to the situtioan surrounding `best_ratio`. That is, when crating a static measure across all years we must only use the years in out `train_set` to create said static measure for the `train_set`. It is important that such static measure does not provide informaiton from the vlaidation/test set back into the training set. For the validation however, we are free to use information from both the training set and the validation set, and for the test set we can freely use information from both train, validation and test set. This might sound a bit convoluted because, well, it is. My best advise to you is simply to think very carefully about what information you would expect to have at your disposal if you where to forecast into the actual future. that is; what information would you have if you did not have a testset?\n",
    "\n",
    "Also, note that when creating our final `test_set` we want to make our validation set part of the training set first (can you see why?). As mentioned, this can be handled by the function below. Specifically by setting `test_time = True`. Don't worry to much about this for now though. We will return to it later; for now it is just important that you understand that this is what is handled in the start of the `featureEng()` function below. Indeed, we will not worry about the test set before we know which features we would like to keep (that is after 4B).\n",
    "\n",
    "As an optional exercise I recommend you to go through the code below and insert some comments regarding what each part does and why that feature might be relevant. Note that I included i two new features `log_best_decay5` and `log_best_decay10` and also made a slight correction to `past_fatalities_country_Npop` , `past_magnitude_country_Npop` and `past_events_country_Npop`. \n",
    "\n",
    "As a last note: **make sure you backup your `train_set.pkl`, `val_set.pkl`, `test_set.pkl` in a safe folder somewhere. Just so you can go get them if you accidentially overwrites the original ones.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay5(data, window=5):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data.iloc[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "def decay10(data, window=10):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    pw0 = alpha * alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = cumsums * scale_arr[::-1]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEng(test_time = False):\n",
    "    \n",
    "    pkl_file = open('train_set.pkl', 'rb')\n",
    "    train_set = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    \n",
    "    if test_time == True:\n",
    "        \n",
    "        pkl_file = open('val_set.pkl', 'rb')\n",
    "        val_set = pickle.load(pkl_file)\n",
    "        pkl_file.close()\n",
    "    \n",
    "        pkl_file = open('test_set.pkl', 'rb')\n",
    "        test_set = pickle.load(pkl_file)\n",
    "        pkl_file.close()\n",
    "    \n",
    "        df = pd.concat([train_set, val_set, test_set])\n",
    "\n",
    "    \n",
    "    elif test_time == False:\n",
    "    \n",
    "        \n",
    "        pkl_file = open('val_set.pkl', 'rb')\n",
    "        val_set = pickle.load(pkl_file)\n",
    "        pkl_file.close()\n",
    "    \n",
    "        df = pd.concat([train_set, val_set])\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"unrecognized input for test_time...\")\n",
    "        \n",
    "    df.sort_values('year', inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "    \n",
    "    df['past_fatalities'] = df.sort_values('year').groupby(['gid'])['best'].cumsum()\n",
    "    df['past_magnitude'] = df.sort_values('year').groupby(['gid'])['log_best'].cumsum()\n",
    "    df['past_events'] = df.sort_values('year').groupby(['gid'])['binary_best'].cumsum()\n",
    "\n",
    "    df['past_fatalities_country'] = df.sort_values('year').groupby(['gwno'])['best'].cumsum()\n",
    "    df['past_magnitude_country'] = df.sort_values('year').groupby(['gwno'])['log_best'].cumsum()\n",
    "    df['past_events_country'] = df.sort_values('year').groupby(['gwno'])['binary_best'].cumsum()\n",
    "        \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "    \n",
    "    features_to_normalize = ['gwarea', 'interp_pop_gpw_sum', 'past_fatalities_country', 'past_magnitude_country', 'past_events_country']\n",
    "\n",
    "    for feature in features_to_normalize:\n",
    "\n",
    "        new_name = 'norm_' + feature\n",
    "        df[new_name] = (df[feature]- df[feature].min())/(df[feature].max()-df[feature].min())        \n",
    "        \n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "\n",
    "    df['past_fatalities_country_Narea'] = df['norm_past_fatalities_country'] / (df['norm_gwarea']+1)\n",
    "    df['past_magnitude_country_Narea'] = df['norm_past_magnitude_country'] / (df['norm_gwarea']+1)\n",
    "    df['past_events_country_Narea'] = df['norm_past_events_country'] / (df['norm_gwarea']+1)\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "    \n",
    "    df['norm_interp_pop_gpw_sum_country'] = df.sort_values('year').groupby(['gwno'])['norm_interp_pop_gpw_sum'].transform(np.sum)#\n",
    "\n",
    "    \n",
    "#     df['past_fatalities_country_Npop'] = df['norm_past_fatalities_country'] / (df['norm_interp_pop_gpw_sum']+1) # so maybe this hsould be country pop not cell pop..\n",
    "#     df['past_magnitude_country_Npop'] = df['norm_past_magnitude_country'] / (df['norm_interp_pop_gpw_sum']+1)\n",
    "#     df['past_events_country_Npop'] = df['norm_past_events_country'] / (df['norm_interp_pop_gpw_sum']+1)\n",
    "\n",
    "    df['past_fatalities_country_Npop'] = df['norm_past_fatalities_country'] / (df['norm_interp_pop_gpw_sum_country'])\n",
    "    df['past_magnitude_country_Npop'] = df['norm_past_magnitude_country'] / (df['norm_interp_pop_gpw_sum_country'])\n",
    "    df['past_events_country_Npop'] = df['norm_past_events_country'] / (df['norm_interp_pop_gpw_sum_country'])\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "    \n",
    "    df['best_decay5'] = df.sort_values('year').groupby(['gid'])['best'].apply(decay5)\n",
    "    df['best_decay10'] = df.sort_values('year').groupby(['gid'])['best'].apply(decay10)\n",
    "    \n",
    "    df['log_best_decay5'] = df.sort_values('year').groupby(['gid'])['log_best'].apply(decay5)\n",
    "    df['log_best_decay10'] = df.sort_values('year').groupby(['gid'])['log_best'].apply(decay10)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "    df['cell_light_Pcap'] = (df['interp_nlights_mean'])/(df['norm_interp_pop_gpw_sum']+1)\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "    \n",
    "    df['interp_nlights_mean_country'] = df.sort_values('year').groupby(['gwno'])['interp_nlights_mean'].transform(np.sum)\n",
    "#     df['norm_interp_pop_gpw_sum_country'] = df.sort_values('year').groupby(['gwno'])['norm_interp_pop_gpw_sum'].transform(np.sum)#\n",
    "\n",
    "    df['country_light_Pcap'] = (df['interp_nlights_mean_country'])/(df['norm_interp_pop_gpw_sum_country']+1)\n",
    "    df['country_light_Area'] = (df['interp_nlights_mean_country'])/(df['norm_gwarea']+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Your comment here:\n",
    "    \n",
    "    df['interp_nlights_mean_norm'] = (df['interp_nlights_mean'] - df['interp_nlights_mean'].min())/(df['interp_nlights_mean'].max()-df['interp_nlights_mean'].min())\n",
    "\n",
    "    df['interp_nlights_mean_country_norm'] = (df['interp_nlights_mean_country'] - df['interp_nlights_mean_country'].min())/(df['interp_nlights_mean_country'].max()-df['interp_nlights_mean_country'].min())\n",
    "    df['country_light_Pcap_norm'] = (df['country_light_Pcap'] - df['country_light_Pcap'].min())/(df['country_light_Pcap'].max()-df['country_light_Pcap'].min())\n",
    "    df['country_light_Area_norm'] = (df['country_light_Area'] - df['country_light_Area'].min())/(df['country_light_Area'].max()-df['country_light_Area'].min())\n",
    "\n",
    "    df['low_ratio_light'] = np.minimum(df['interp_nlights_mean_country_norm'] / df['interp_nlights_mean_norm'],1)  \n",
    "    df['low_ratio_light_Pcap'] = np.minimum(df['country_light_Pcap_norm'] / df['interp_nlights_mean_norm'],1)  \n",
    "    df['low_ratio_light_Area'] = np.minimum(df['country_light_Area_norm'] / df['interp_nlights_mean_norm'],1)\n",
    "    \n",
    "    last_year = df['year'].max()\n",
    "    \n",
    "    train_set = df[df['year'] < last_year]\n",
    "    other_set = df[df['year'] == last_year] # val or test set\n",
    "\n",
    "    return train_set, other_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = featureEng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle test with all the features\n",
    "\n",
    "file_name = \"train_set_featureEng.pkl\"\n",
    "output = open(file_name, 'wb')\n",
    "pickle.dump(train_set, output)\n",
    "output.close()\n",
    "\n",
    "#pickle val with all the features\n",
    "\n",
    "file_name = \"val_set_featureEng.pkl\"\n",
    "output = open(file_name, 'wb')\n",
    "pickle.dump(val_set, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to 4B! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
