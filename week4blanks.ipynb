{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.bunus\n",
    "\n",
    "Before we get started with the actual exercise, I think it prudent to have a small (really small) torturial on \"for loop\"s. It seemed a lot of you had some problems in 1B because the mechanics of the for loop was not clear. I'll try to ammend this here. If you are comfortable in your for loops, feel free to skip this part.\n",
    "\n",
    "In a for loop you iterate over a sequence (a list, an array, a tuple, a dictionary, a set, a string or what not.). Think of it as saying:\n",
    "\n",
    "`\"once, for each item in this sequence do: ...\"`   \n",
    "\n",
    "What you denote the item is up to you - it could be i, j, k, n or some more substantial name such as \"feature\". Naturally there are conventions and I would recommend that you keep your code as readable as possible, so i or somthing substial meaningfull is a good start.\n",
    "\n",
    "Make some list `[]` or np.array below and put whatever in it - just not to long. Call the list somthing like \"a_list\" (or an_array if you do this with a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a for loop where you iterate over this list and print each element. Remember, you choose what to denote the items you are iterating over. So:\n",
    "\n",
    "`for ... in a_list:\n",
    "    print(...)`\n",
    "    \n",
    "The only thing needed here is consistency in what you call the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can also make operation inside the for loop. Use the array (some_array) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "some_array = np.array([1,9,4,16,25,49,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a for loop where you take the square root (`np.sqrt()`) of each item in your list and assign (`=`) the result to a new variable `out`. Then print `out`. It should look somthing like this:\n",
    "\n",
    "`for ... in some_array:\n",
    "    ... = np.sqrt(...)\n",
    "    print(...)`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want both the items of your sequence and the index of the sepcific items (that is, what number in the sequence is the item). Use the function below to print i and j.\n",
    "\n",
    "`for i,j in enumarate(some_array):\n",
    "    print(...,...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does that make sense?   \n",
    "Now, combining the two for loops above, make a loop that prints both the index of and item and the sqrt of said item (be careful that you don't print the sqrt of the index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, try to put some for loop into a function (as shown in 2A). Make a function that takes a list/array and outputs or prints some transformation of this list/array. If you decide this transformation should use one or more extra parameters - however, make sure to add these as parameters to you function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last note. Some times we use for loops to print stuff, but often we use it for things much more complex. That being said, printing out what your are actually looping over is always a good idea when constructiong a loop or debugging. If a loop  does not work, you first step should be making sure, that what your are looping over makes sense, and that can be done by printing the individual items as you loop over them.\n",
    "\n",
    "This was a very short - highly - presentation of for loops. There a plenty of reasouces only e.g. https://www.pythonforbeginners.com/basics/loops/ you can browse for way more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.for real:\n",
    "\n",
    "The focus in this assigment will be different evaluations metrics - perticularly in regards to imbalanced data. Having read **He & Garcia 2008 Learning from imbalanced data** (especailly section 4 and onwards) will be a huge help here. We will evaluate the results we got from the ratio- and persistance \"model\" in 2A.\n",
    "\n",
    "A central issue when using machine learning for conflict prediction is the fact that the data is always imbalanced. That is, most of the time, most places do not experience conflict. Or said another way, our target \"binary best\" is mostly zeroes with only relatively few ones. This is a problem since most machine learning algorithms for classification assumes, or even expects, balanced data, i.e. roughly an equal amount of zeros on one's. One possible consequence of this is that our models might end up just predicting the majority class for every instance. To understand why, remember how the metric accuracy was defined in 2A:\n",
    "\n",
    "$$\n",
    "\\frac{TP + TN}{TP + TN + FP + FN} = \\frac{P_c}{N}\n",
    "$$\n",
    "\n",
    "Machine Learning generally works by minimizing some error (we return to this later in the course). Accuracy is one way to measure error - or the lack here of. We could take the inverse of accuracy ($1-acc$) to get an actual measure of error if we wanted to. Now imagine we ask some algorithm to keep optimizing itself with the goal of maximizing the accuracy (or correspondingly minimizing the error). What would happen if our data were $95\\%$ zeros and $5\\%$ ones? Think about it for a second.\n",
    "\n",
    "Well, if our algorithm is a bit lazy (it probably is) it could just decide to classify everything as zeros. This would give it and accuracy of $0.95$ (where 1 is the max). If this does not make sense to you, look at the equation above and think about it. If $95\\%$ of the data are zeros and I classify everything as zeros, then I will get $95\\%$ of the observations as correct $P_c$ and dividing with $N$ would leave us the result $0.95$. You can try with some toy numbers if you need to convince yourself. \n",
    "\n",
    "As you might exepct, the imbalance of the data is also one reason the ratio- and persistance \"model\" to get so impressive results. Most cells did not expericence conflict most of the time. The ratio- and persistance \"model\" captures this just fine (There are another reason why they are doing alright, which we will return to later in the course.). \n",
    "\n",
    "Thus, we need are evaluation metrics which - contrary to accuracy - gives honest evaluations of our model's performance on unbalanced data. First, when we actuelly have honest evaluations, does it make sense to start thinking about choosing/creating models which are robust against the imbalance of the data. We will focus on finding the right evaluation metrics here and return to choosing/creating imbalanced-robust models in a later exercice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take it one step at a time, but start by importing all the libraries we need: numpy as np, pandas as pd, geopandas as gpd, matplotlib.pyplot as plt and pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will also need the machine learning library **scikit learn** https://scikit-learn.org/stable/index.html. You might need to install it and you might not - try importing it first. If you do need to install it then, **in your terminal after having activated your geo_env**, run:\n",
    "\n",
    "`conda install scikit-learn`\n",
    "\n",
    "And now you should be able to also import `sklearn`. Here however we only need the metrics part of the library. So just run:\n",
    "\n",
    "`from sklearn import metrics` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.0 \n",
    "\n",
    "As usual we need to load our pickle(s). We will not need the validation or test set yet, so just load your train_set pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make whatever tests and plots your need to insure yourself that everything is alright. (if you plot different years remember to set vmin and vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.2 Accuracy\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy#sklearn.metrics.accuracy_score\n",
    "\n",
    "Now, lets start where we left of; with accuracy. It might not be a good measure of imbalanced data, but we should know it nontheless. \n",
    "\n",
    "For all assigments below we will, for simplicity, settle for evaluating our predicting one year into the future. That is, we will only evaluate the persistence and ratio \"model\" against binary_best_t1. Remember, that the persistence model simply entails using binary_best to predict binary_best_tx (here t1) and the ratio model entails using binary_ratio to predict binary_best_tx (here t1).\n",
    "\n",
    "Remember the formula for accuracy:\n",
    "\n",
    "$$\n",
    "Acc = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{P_c}{N}\n",
    "$$\n",
    "\n",
    "So let's impliment that - and let's make what we do very explicit. First let's assign our prediction of y `train_set['binary_best']` to a variable `y_pred` and our target `train_set['binary_best_t1']` to a variable `y_true`:\n",
    "\n",
    "`y_pred = ...`    \n",
    "`y_true = ...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get $P_c$. Having two numpy arrays or pandas series, we can compare them with the following code:\n",
    "\n",
    "`y_pred == y_true`\n",
    "\n",
    "Here, the fact that we use two `==` is central. It is like asking if and where these arrays are equal, not assigning/telling them to be equal. So what you get is an array of True/False. True being where we predicted correct (so y_pred was equal to y_true) and false where we missed (so y_pred was *not equal* to y_true).Thus, remembering python will interpret True as a $1$ and False as a $0$ we can take the `.sum()` of this expression to get $P_c$:\n",
    "\n",
    "`Pc = (... == ...)....`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we just need $N$. This is easy, since it is just the number of observations in our dataset. You can get that number from `y_pred`, `y_true` or even the `train_set` by calling the `.shape` method and taking the first entry `[0]` here (the first is number of rows, aka observations, the second is number of columns aka features.):\n",
    "\n",
    "`N = ...[0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it together and assign it to a variabel called `acc`:\n",
    "\n",
    "$$\n",
    "acc = \\frac{P_c}{N}\n",
    "$$\n",
    "\n",
    "Then print acc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify, try using sklearns accuracy metric:\n",
    "`metrics.accuracy_score(y_true, y_pred)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it correspond? It should.   \n",
    "So you might ask why should we do it manually, when you can just use sklearn? In practice, you will often use a library such as sklearn to calculate your metrics - espacially when they get more complicated than accuracy. But it is important to have an intuition about what the metric actually does, and that intuition is best achived by doing the math. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.3 False Positives, False Negatives, True Positives, True Negatives and the Confusion Matrix.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So hopefully all of you have an idea of what a False Positives (FP), False Negatives (FN), True Positives (TP) and True Negatives (TN) are. If not, just go ask the internet. What, perhaps, is more arcane is the term *Confusion Matrix* (even when you know what it is, the term is not really helpful). The Confusion Matrix is simply a $4\\times4$ matrix (since our task is binary) denoting the counts of FP, FN, TP and TN.  \n",
    "\n",
    "Let's start by calculating TP. Using what we learned above, this is quite easy. Think about what you want: You want all instances where `y_pred == 1` **and** `y_true == 1`. Then you want the `.sum()` of all this. The code will fit in this template:\n",
    "\n",
    "`TP = ((... == ...) & (... == ...))....`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the same for FP, TN and FN. Here, you just have to think about what you want `y_pred` to equel, and what you want `y_true` to equel. For instance, for `FP`, we would like all instances where we predicted conflict (`y_pred == 1`) but no conflict actaullt manifested (`y_true == 0`). And so on and so forth. Remember to take the `.sum()` of the expressions.\n",
    "\n",
    "`FP = ((y_pred == 1) & (y_true == 0))....`     \n",
    "`TN = ((... == ...) & (... == ...))....`   \n",
    "`FN = ((... == ...) & (... == ...))....`   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try print and interpret the TP, FP, TN and FN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a confusion matrix just put it all together in a np.matrix like this:\n",
    "\n",
    "`CM1 = np.array([[TN,FP],[FN,TP]])`\n",
    "\n",
    "and print CM1 (Or make a fancy plot if you want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also just use sklearn. Try using the `metrics.confusion_matrix()` function with inputs `y_true` and `y_pred`. Save the result as CM2. and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per convention the count of True Negatives is $CM_{0,0}$, False Negatives is $CM_{1,0}$, True Positives is $CM_{1,1}$ and False Positives is $CM_{0,1}$. Here, the first subscript denotes the row and the second denotes the column of the confusion matrix. Thus, we can extract the FP, TP, TN and FN  values from the sklearn matrix if we whant to. Try extracting the values like:\n",
    "\n",
    "`\n",
    "TN = CM[0][0]\n",
    "FN = CM[...][...]\n",
    "TP = CM[...][...]\n",
    "FP = CM[...][...]\n",
    "`\n",
    "\n",
    "And print them one more time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully your (CM1) and sklearn (CM2) results are the same.\n",
    "\n",
    "**What do you get most of?**  \n",
    "**How can that explain the high acc score?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.4 Recall and Precision score\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall#sklearn.metrics.recall_score   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision#sklearn.metrics.precision_score\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall.\n",
    "\n",
    "\n",
    "Now, i'm not gonna go into a lot of details regarding the interpretations of this metric, as it is better to read about it in He & Garcia or one of the links above. To put it breifly, precision denotes the share of classified positives which were actual true positives, while recall denotes the share of all actual positives we manage to capture: \n",
    "\n",
    "$$\n",
    "precision = \\frac{TP}{TP+FP}; \\quad recall = \\frac{TP}{TP+FN} \\tag{16} \\label{eq:recall/precision}\n",
    "$$\n",
    "\n",
    "These two measures are both relevant when dealing with imbalanced data. this is because they measure how well we predict events (here the minority class conflicts) and not the absence of events (here the majority class not-conflict).\n",
    "\n",
    "Given that we have already calculated TP, FP, TN and FN, you should have no problem calculating precision and recall below:\n",
    "\n",
    "`precision_score = ...\n",
    "recall_score = ...`\n",
    "\n",
    "and then print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the sklearn implimentaions `metrics.precision_score()` and `metrics.recall_score()` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you get the exact same results. Now since these scores also goes from 0 to 1, this appears as a somewhat less impressive result compared to the accuracy score.\n",
    "\n",
    "**Can you interpret recall more substantially?**   \n",
    "**Can you interpret precision more substantially?**  \n",
    "**Why are these scores so much lower than acc?**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.5 The ROC curve and the AUC score\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html   \n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic \n",
    "\n",
    "Now, as you might have noticed we have only tested the persistence model. We have not yet looked at the ratio model. As you might remember from the last exercise ratio is, well, a ratio. So instead of being 0 or 1 (which we denote $y_{pred}\\in \\{0,1\\}$) it is a value between 0 and 1 (which we denote $y_{pred}\\in (0,1)$). So in order to compare y_pred from the ratio model to y_true, we need to set a threshold. We could decide that all observations with a ratio above 0.5 (y_pred > 0.5) becomes 1 and all observations with a ratio including and below 0.5 become 0 (y_pred <= 0.5). Doing this would turn our ratio binary and thus we could compare it to y_true as we did above with the persistence model. Naturally, this is also how we would handle estimated probabilities which are also values between 0 and 1.\n",
    "\n",
    "The problem is, that thresholds (e.g. 0.5) often appear abitrary and add hoc. Yes, 0.5 might seem reasonable, but really it could be whatever. We might want our framwork to be very sensitive to possible positives so we could set the threshold lower - or higher if we want a less sensitive framework. Or perhaps some thresholds are simply best if we want to maximise the number of correctly predicted units $P_c$.\n",
    "\n",
    "So how do we handle this? Well, we just evaluate the model at a lot of different thresholds. One way to do this is via the *Receiver Operating Characteristic curve*, also called the ROC curve. This metric is described in both He & Garcia 2008 and Hastie et al. 2001, thus I will only outline it breifly here. The ROC curve denotes the trade-off between the *true positive rate* ($TP_{rate}$) and the *false positive rate* ($FP_{rate}$) by plotting the $TP_{rate}$ over the $FP_{rate}$ given different thresholds. Thus The ROC curve circumvents the issue of a hard thresholds by evaluating our results at many possible thresholds. Denoting the actual number of negatives $N_C$, the rates can be expressed as such:\n",
    "\n",
    "$$\n",
    "TP_{rate} = \\frac{TP}{P_C};\\quad FP_{rate}=\\frac{FP}{N_C} \\tag{15} \\label{eq:TPFP}\n",
    "$$\n",
    "\n",
    "As we can see from this equation the $TP_{rate}$ is simply an other name for recall (it is totally alright if this is not clear, but then spend a minute convincing youself why this is.) \n",
    "\n",
    "\n",
    "We will not spend time doing this manually - it is not convoluted, just a bit bothersome since we have derive $TP_{rate}$ and $FP_{rate}$ over a lot of thresholds either by for loop or vectorization. As such, we skip straight to the sklearn implimentation. \n",
    "\n",
    "First, however we will want to switch from evaluate the persistence model to the ratio model (as seen the persistence model does not need a threshold since it is already binary, so we would miss the point if we keept that as an exemple). create a new `y_pred` which is given by `train_set['ratio_best']`\n",
    "\n",
    "`y_pred = ...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the sklearn funcitons above `metrics.roc_curve()` simple takes two arguments; `y_true` and `y_pred`. But unlike the other sklearn functions `metrics.roc_curve()` gives three outputs `FP_rate`, `TP_rate` and `thresholds`. That is easily handled thus:\n",
    "\n",
    "`FP_rate, TP_rate, thresholds = metrics.roc_curve(..., ...)`\n",
    "\n",
    "(we will not use the `thresholds` for anything explicitly here. But you can browse them if you like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the ROC curve. Simply use `plt.plot()` where the first entry should by `FP_rate` and the second `TP_rate`.\n",
    "\n",
    "`plt.plot(..., ...)`\n",
    "\n",
    "We can also easily plot the random model the the ROC curve, since that would just constitute a straight diagonal. We can plot such a line thus:\n",
    "\n",
    "`plt.plot([0, 1], [0, 1], 'k--')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(..., ...)\n",
    "plt.plot(..., ..., 'k--')\n",
    "\n",
    "plt.title('roc curve')\n",
    "plt.xlabel('FP_rate')\n",
    "plt.ylabel('TP_rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That (hopefully) looks very promising. The ROC curve can be interpreted visually or summarized by the area under it; the AUC (Area Under the Curve) score. Again, we will jump straight to the sklearn implimentaion of AUC `metrics.roc_auc_score()` which works just as you would expect (with inputs `y_true` and `y_pred`). Save it as `AUC` and print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve and AUC score have been widely used in conflict studies (chadefaux 2017), and was previously coined as the \"gold-standard\" in this field (Perry 2013). However, as you might have read in He & Garcia 2008, it is not without its issues. Especially when it comes to impbalanced data. Specifically the ROC curve and AUC score tends to judge model-performance on highly imbalanced data overly favourable (He & Garcia, 2008).\n",
    "\n",
    "**Try looking at the equations for $TP_{rate}$ and $FP_{rate}$ - can you see why alot of (easy to predict) TN's might compromise the metric?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply seeing the ROC curve and the AUC score might also have stimulated your skepticism. Compared to the Recall and Precision the AUC does appear suspiciously generous. Despite this weakness the ROC curve and AUC score are still routinely reported. The simple reason is that these metrics have been widely used and are rather commonly known. The ROC curve and AUC score are points of reference for most political scientist working with advanced quantitative methods and including these metrics will makes it easier to compare novel projects with past efforts. That being said, we should move on to more appropriate metrics better suited for highly imbalanced data.  \n",
    "\n",
    "Luckly, there are similar metrics better suited for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.6 The Precision Recall Curve and the Average Precision score\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html?highlight=precision#sklearn.metrics.precision_recall_curve   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score\n",
    "\n",
    "The Precision Recall (PR) Curve is very similar to the ROC curve except - as you might deduce from the name - it plots Recall over Precision, instead of $TP_{rate}$ over $FP_{rate}$. We already know how to calculate Recall and Precision and will not bother going over different thresholds, so we just go straight to the sklearn implimentation `metrics.precision_recall_curve()`. This function works as expected (with `y_true` and `y_pred` as inputs) and as with the ROC function, this function also generates three outputs: `precision`, `recall` and `thresholds`. Create these three outputs:\n",
    "\n",
    "..., ..., ... = ... \n",
    "\n",
    "(notice here, that you get an array of precisions and an array of recall corrospoding to the different thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot precision over recall (as you did above with $TP_{rate}$ over $FP_{rate}$ for the ROC curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(..., ...)\n",
    "\n",
    "plt.title('PR curve')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As with the ROC curve, the PR curve can be interpreted visually and also be summarized in a single measure. In the case of the PR curve this measure is called Average Precision (AP). The AP score denotes a weighted mean of precision at each possible threshold. The weighting is the increase in recall from the previous threshold. Denoting recall $R$, precision $P$, and the different thresholds as $n$ the AP score can be expressed as seen:\n",
    "\n",
    "$$\n",
    "AP = \\sum_n (R_n-R_{n-1})P_n\n",
    "$$\n",
    "\n",
    "We, however, will skip straight to the sklearn implimentation `metrics.average_precision_score()`. It works as you would expect (with inputs $y_true$ and $y_pred$). Save the output to a variabel called AP and print it:\n",
    "\n",
    "`AP = ...   \n",
    "print(AP)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AP score has a number of similarities with AUC and can be thought of as the area under the PR curve (Su 2015). The key difference is that AP places more emphasis on identifying high probability events than identifying low probability events. The interpretation also differs from AUC. An AUC score of 0.5 denotes a classifier which is no better than random. This is not the case with AP where 0.5 can be a decent score depending on the context. Specifically, an approximation of the \"random\" baseline of AP is given by the share of events versus non-events (Bestgen 2015). If the data is precisely balanced the baseline for the AP score is 0.5 just like the AUC score. If the data is imbalanced, say $5\\%$ events versus $95\\%$ non-events, the baseline for the AP score will be 0.05. As such, AP is a good evaluation metric for judging and comparing the predictive performance of models on imbalanced data without setting a hard threshold.\n",
    "\n",
    "So can we interpreted it more substantially? No, not really. The AP score does not have an intuative substantial interpretation beyond what is given above (Hegre et al. 2019). Can we then say whether this AP score is good or bad? Well, of course we know an AP score of 1 would mean we solved the task perfectly, but for many tasks such a score is highly unrelalistic. As such, it is hard too determined whether a AP score is good or bad whitout context: Which is why we have baseline models! Going forward we can compare new models to the random model, the persistence model and the ratio model and see how the AP score of more complex models fare against these very simple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B.7 The Brier score (or loss)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html?highlight=brier   \n",
    "https://en.wikipedia.org/wiki/Brier_score\n",
    "\n",
    "The Brier score is the last score we will (very briefly) look at here. It is a score which is also used somewhat often in conflict prediction/forecasting. Like the last couple of metrics the Brier score does not need a hard threshold. But unlike the metircs above this is actually a *loss* which means that the lower the score the better. The Brier score is given by:\n",
    "\n",
    "$$\n",
    "BS = \\frac{1}{N}\\sum^{N}_{i=1} (y_{pred(i)} - y_{true(i)})^{2}\n",
    "$$\n",
    "\n",
    "One argument for using the Brier score is that it \"*favors sharp, accurate probabilistic predictions (near 0 or 1)*\" (Hegre et al. 2019). Thus a model with a good (low) brier score will be relativly safer for relavant actors to use as a guide for action/policy (at least in theory). A counter argument would be that we can just set a threshold somewhere and squeeze the values (using some funciton) on each side of this threshold further to the extremes. This might give us a better Brier socre, but it has not really changed how well our model actually performs. Never-the-less, it has other nice properties and is used as a reference in most modern conflcit prediction.\n",
    "\n",
    "We will just use the sklearn implmentation `metrics.brier_score_loss()` which works as you might expect. Save the output to BS and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the last couple of metrics, the Brier is also a bit hard to interpret and is often used as a relative metric, comparing models against each other or against baselines, rather than as an \"absolute\" metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing up\n",
    "\n",
    "These are just some of the metrics out there and indeed we only here looked at metrics for classification without even touching on metrics for regression. You might wonder which of these metrics to use when reporting you predictions. Well, it is has been argued that using multiple metrics to evaluate model performance is necessary, since model performance is inherently multi-dimensional (Hegre et al, 2019). I agree with that assessment and moving forward we will see how a combination of metrics can help convey our result. Looking at the most reason puplication is seems the standart right now is to report ROC/AUC, PR/AP and Brier. I also find it very helpful and illuminating to plot the $TP$, $FP$, $TN$ and $FN$ out on a map. That way I can see where my model does well and where it fails in more substantial terms. \n",
    "\n",
    "Another reason to include more metrics is simply comparability. New and better metrics are discovered and formulated periodically, but we still need to compare our efforts (when at all possible) with past efforts. This means including some less then optimal metrics - e.g. the ROC curve and AUC score. Naturally we should still be honest about the weaknesses of such metrics and underline which metrics we think gives the most honest evaluation of our models (which will often be the PR curve and the AP score in our case).\n",
    "\n",
    "**And you'r done - cheers!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
